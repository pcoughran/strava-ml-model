{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m### Read and clean CSV's\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Variables\u001b[39;00m\n\u001b[1;32m      6\u001b[0m cutoff_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Read and clean CSV's\n",
    "\n",
    "# Variables\n",
    "cutoff_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Read CSV's\n",
    "activities_df = pd.read_csv('activities.csv')\n",
    "ftp_df = pd.read_csv('ftp.csv')\n",
    "\n",
    "# Drop unnessary columns\n",
    "activities_df.drop(['Activity Description','Commute', 'Activity Private Note', 'Activity Gear', 'Filename', 'Athlete Weight', \n",
    "         'Bike Weight', 'Weather Observation Time', 'Weather Condition', 'Weather Temperature', 'Apparent Temperature',\n",
    "         'Dewpoint', 'Humidity', 'Weather Pressure', 'Wind Speed', 'Wind Gust', 'Wind Bearing', 'Precipitation Intensity',\n",
    "         'Sunrise Time', 'Sunset Time', 'Moon Phase', 'Gear', 'Precipitation Probability', 'Precipitation Type', \n",
    "         'Cloud Cover', 'Weather Visibility', 'UV Index', 'Weather Ozone', 'Jump Count', 'Total Grit', 'Average Flow', \n",
    "         'Flagged', 'Dirt Distance', 'Newly Explored Distance', 'Newly Explored Dirt Distance', 'Total Steps', \n",
    "         'Carbon Saved', 'Pool Length', 'Timer Time', 'Media', 'Total Weight Lifted', 'From Upload', 'Commute.1', \n",
    "         'Average Positive Grade', 'Average Negative Grade', 'Average Grade Adjusted Pace', 'Total Cycles', \n",
    "          'Number of Runs', 'Uphill Time', 'Downhill Time', 'Other Time',], axis=1, inplace=True)\n",
    "\n",
    "# Only keep Rides\n",
    "activities_df = activities_df[activities_df['Activity Type'] == 'Ride']\n",
    "\n",
    "# Convert to datetimes and numeric\n",
    "activities_df['Activity Date'] = pd.to_datetime(activities_df['Activity Date'], errors='coerce')\n",
    "ftp_df['FTP Test Date'] = pd.to_datetime(ftp_df['FTP Test Date'], errors='coerce')\n",
    "activities_df['Distance'] = pd.to_numeric(activities_df['Distance'], errors='coerce')\n",
    "\n",
    "# Sort both dataframes by date\n",
    "ftp_df = ftp_df.sort_values(by='FTP Test Date')\n",
    "activities_df = activities_df.sort_values(by='Activity Date')\n",
    "\n",
    "# Only look at data starting 2023-01-01\n",
    "ftp_df = ftp_df[ftp_df['FTP Test Date'] >= cutoff_date]\n",
    "activities_df = activities_df[activities_df['Activity Date'] >= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "### Create training blocks\n",
    "\n",
    "# Define thresholds for Zone 2 and high-intensity rides\n",
    "max_hr = 196  # Replace with your actual max heart rate if available\n",
    "high_intensity_min_hr = max_hr * 0.95\n",
    "\n",
    "# Initialize an empty list to store training blocks\n",
    "training_blocks = []\n",
    "\n",
    "# Iterate through the FTP test data to create training blocks\n",
    "for i in range(len(ftp_df) - 1):\n",
    "    # Define the start and end date of the block\n",
    "    start_date = ftp_df.iloc[i]['FTP Test Date']\n",
    "    end_date = ftp_df.iloc[i + 1]['FTP Test Date']\n",
    "    \n",
    "    # Filter activities within the training block\n",
    "    block_activities = activities_df[(activities_df['Activity Date'] >= start_date) & \n",
    "                                     (activities_df['Activity Date'] < end_date)]\n",
    "    \n",
    "    # Calculate block duration in weeks\n",
    "    block_duration_weeks = (end_date - start_date).days / 7\n",
    "    \n",
    "    # # Calculate aggregated metrics for the block\n",
    "    # avg_relative_effort = block_activities['Relative Effort'].mean()\n",
    "    # avg_weighted_watts = block_activities['Weighted Average Power'].mean()\n",
    "\n",
    "    # Calculate longest ride time\n",
    "    longest_ride_duration = block_activities['Moving Time'].max()\n",
    "    \n",
    "    # Classify Zone 2 and high-intensity rides\n",
    "    zone_2_rides = block_activities[block_activities['Max Heart Rate'] < high_intensity_min_hr]\n",
    "    high_intensity_rides = block_activities[block_activities['Max Heart Rate'] >= high_intensity_min_hr]\n",
    "    \n",
    "    # Calculate the ratio of Zone 2 and high-intensity rides\n",
    "    total_rides = len(block_activities)\n",
    "    zone_2_ratio = len(zone_2_rides) / total_rides if total_rides > 0 else 0\n",
    "    high_intensity_ratio = len(high_intensity_rides) / total_rides if total_rides > 0 else 0\n",
    "    \n",
    "    # Calculate rides per week\n",
    "    rides_per_week = total_rides / block_duration_weeks if block_duration_weeks > 0 else 0\n",
    "\n",
    "    # Calculate distance per week\n",
    "    km_per_week = block_activities['Distance'].sum() / block_duration_weeks if block_duration_weeks > 0 else block_activities['Distance'].sum()\n",
    "\n",
    "    # # Calculate Variability Index: ratio of normalized power to average power within the block (try to find interval training)\n",
    "    # avg_power = block_activities['Average Watts'].mean()\n",
    "    # norm_power = block_activities['Weighted Average Power'].mean()  \n",
    "    # variability_index = norm_power / avg_power if avg_power > 0 else 0\n",
    "\n",
    "    # Calculate FTP change during the block\n",
    "    ftp_change = ftp_df.iloc[i + 1]['FTP'] - ftp_df.iloc[i]['FTP']\n",
    "    \n",
    "    # Store block information\n",
    "    training_blocks.append({\n",
    "        'Block Duration (weeks)': block_duration_weeks,\n",
    "        'Rides Per Week': rides_per_week,\n",
    "        'KM Per Week': km_per_week,\n",
    "        # 'Average Relative Effort': avg_relative_effort,\n",
    "        # 'Average Weighted Watts': avg_weighted_watts,\n",
    "        # 'Variability Index': variability_index,\n",
    "        'Zone 2 Ratio': zone_2_ratio,\n",
    "        'High Intensity Ratio': high_intensity_ratio,\n",
    "        'Log Longest Ride': np.log1p(longest_ride_duration),\n",
    "        'FTP Change': ftp_change\n",
    "    })\n",
    "\n",
    "# Convert training blocks to a DataFrame\n",
    "training_blocks_df = pd.DataFrame(training_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the training blocks\n",
    "\n",
    "training_blocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for training\n",
    "\n",
    "# Prepare the feature set and target variable\n",
    "X = training_blocks_df.drop(columns=['FTP Change'])\n",
    "y = training_blocks_df['FTP Change']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Try hyperparameter Randomized grid search\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Define the parameter grid for RandomForestRegressor\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],       # Number of trees in the forest\n",
    "#     'max_depth': [5, 10, 15, None],        # Maximum depth of the tree\n",
    "#     'min_samples_split': [2, 5, 10],       # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4],         # Minimum number of samples required to be at a leaf node\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "# }\n",
    "\n",
    "# # Initialize the RandomForestRegressor\n",
    "# rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Set up the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "#                            cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "# # Fit GridSearchCV to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# # Fit the best model to the training data\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "# y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the best model's performance\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a Random Forest Regressor\n",
    "\n",
    "# Initialize and train a Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ Random Forest Regressor  ------ \")\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Feature importancech\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and train a LightGBM Regressor\n",
    "model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ LightGBM Regressor  ------ \")\n",
    "\n",
    "print(f\"LightGBM Mean Squared Error: {mse}\")\n",
    "print(f\"LightGBM R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize and train an XGBoost Regressor\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ XGBoost Regressor  ------ \")\n",
    "\n",
    "print(f\"XGBoost Mean Squared Error: {mse}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# # Initialize and train a CatBoost Regressor\n",
    "# model = CatBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42, verbose=0)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\" ------ CatBoost Regressor  ------ \")\n",
    "\n",
    "# print(f\"CatBoost Mean Squared Error: {mse}\")\n",
    "# print(f\"CatBoost R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize and train an ElasticNet Regressor\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ ElasticNet Regressor  ------ \")\n",
    "\n",
    "\n",
    "print(f\"ElasticNet Mean Squared Error: {mse}\")\n",
    "print(f\"ElasticNet R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the feature set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train a Support Vector Regressor\n",
    "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ Support Vector Regressor  ------ \")\n",
    "\n",
    "print(f\"SVR Mean Squared Error: {mse}\")\n",
    "print(f\"SVR R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Initialize and train an MLP Regressor\n",
    "model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" ------ MLP Regressor  ------ \")\n",
    "\n",
    "print(f\"MLP Mean Squared Error: {mse}\")\n",
    "print(f\"MLP R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Plot charts\n",
    "\n",
    "# Plot FTP Over Time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ftp_df['FTP Test Date'], ftp_df['FTP'], marker='o', linestyle='-', color='b')\n",
    "plt.title('FTP Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('FTP (Watts)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot features, ranked by feature importance\n",
    "for feature, importance in feature_importance.items():\n",
    "    print(f\"Importance of {feature}: {importance}\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(training_blocks_df.index, training_blocks_df[feature], marker='o', linestyle='-', color='purple')\n",
    "    plt.title(f\"{feature} by Training Block\")\n",
    "    plt.xlabel('Training Block')\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
